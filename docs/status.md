---
layout: default
title: Status
---
## Watch the Status Report
[![Watch the Status Report](https://img.youtube.com/vi/kHjt8DJ0yOw/maxresdefault.jpg)](https://youtu.be/kHjt8DJ0yOw) 

## Project Summary
Our agent will start in a 50x50 world with 15 pigs spawned randomly throughout the world, alongside various sparse obstacles including lava, blocks, or trees. The goal is for our agent to track and reach pigs before falling victim to traps, running out of time, or otherwise dying. Since our agent really wants to be "friends" with pigs, and not sheep, chicken, or cows, the mission only successfully ends when it captures pigs. This contrasts with our initial idea, which was to learn how to mine a tree in a plain 10x10 dirt floating island.

## Approach
For our general approach we have used a Q Network to determine the best action given the environment. The network is updated using adaptive moment estimation (ADAM). Adam uses estimates of the first and second moments of the gradient. This is done using mt=βmt + (1−β)gt for the first moment and vt = βvt + (1−β)tg^2 for the second moment. β is the rate of decay for the past estimates. The moments are then divided by 1 - β and then used to update the network using θt = θt − η/(vt^(1/2)+ϵ) * mt.

The network takes the rgb input as 3 channels of 800 by 500, supplied by a ColourMap Video Producer. This gives an image where each unique block type is given its own flat color instead of a complex texture. The input is then modified through a series of two convolutional layers, dropout, and two fully connected layers. The network gives three different values as output, each corresponding with a different action. The 3 actions are "move 0.5", "turn 0.25", and "turn -0.25". We learned that the agent wouldn't need to stop when moving towards the tree and wanted to simplify the model as much as possible so we chose to stick with just these three outputs. The speeds of the actions are lowered from one in order to obtain more input frames per action interval. The agent is allowed to move continuously through the environment using these actions. The networks receives negative rewards when the agent moves off the platform and positive rewards for when the agent looks at or touches the tree. The network also receives a reward based on the distance between the agent and the tree.

We have tried several modifications to improve our performance. One modification was to add more trees to the environment and slowly reduce the number of trees as training continues. The rational behind this decision was that using a single tree means that the agent isn't very likely to come in contact with the tree. By increasing the reward, the agent will come in contact with more trees which could reduce the amount of time needed for the agent to make the correlation between a tree and a positive reward. Another modification was to remove pooling and simply use larger kernel sizes in the convolutional layers. This modification reflects the current trend in state of the art systems. We also tried adding a barrier with a negative reward to the edge of the environment so that the agent would be able to train longer instead of falling off the side. One of the more effective modifications we made was to the rewards provided to the network. At first we had a -10000 reward for falling off the edge and a 10000 reward for touching the tree. This led to the agent prioritizing staying on the platform instead of trying to reach the tree. Instead of having a moving agent we instead had an agent that would spin in circles for the duration of each episode. After lowering the reward for falling off the edge to -200, we were able to get a better performing agent. 

## Evaluation
# An important aspect of your project, as we mentioned in the beginning, is evaluating yourproject. Be clear and precise about describing the evaluation setup, for both quantitative and qualitativeresults. Present the results to convince the reader that you have aworkingimplementation. Use plots, charts,tables, screenshots, figures, etc. as needed. I expect you will need at least a 1-2 paragraphs to describe eachtype of evaluation that you perform.


## Remaining Goals and Challenges
# In a few paragraphs, describe your goals for the next 4-5 weeks, whenthe final report is due. At the very least, describe how you consider your prototype to be limited, and whatyou want to add to make it a complete contribution. Note that if you think your algorithm is quite good,but have not performed sufficient evaluation, doing them can also be a reasonable goal. Similarly, you maypropose some baselines (such as a hand-coded policy) that you did not get a chance to implement, butwant to compare against for the final submission. Finally, given your experience so far, describe some of thechallenges you anticipate facing by the time your final report is due, how crippling you think it might be,and what you might do to solve them

## Resources used
# Mention all the resources that you found useful in writing your implementation. This shouldinclude everything like code documentation, AI/ML libraries, source code that you used, StackOverflow, etc.You do not have to be comprehensive, but it is important to report the ones that are crucial to your project. Iwould like to know these so that the more useful ones can be shared with others in the course
